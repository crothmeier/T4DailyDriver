apiVersion: v1
kind: Namespace
metadata:
  name: vllm-production
---
# Legacy deployment (95% traffic)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-legacy
  namespace: vllm-production
  labels:
    app: vllm
    version: legacy
    cuda: "12.1"
    pytorch: "2.3.1"
    vllm: "0.5.3"
spec:
  replicas: 3
  selector:
    matchLabels:
      app: vllm
      version: legacy
  template:
    metadata:
      labels:
        app: vllm
        version: legacy
        cuda: "12.1"
        pytorch: "2.3.1"
        vllm: "0.5.3"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      nodeSelector:
        nvidia.com/gpu: "true"
        node.kubernetes.io/instance-type: "g4dn.xlarge"  # T4 GPU nodes
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
      containers:
      - name: vllm
        image: ghcr.io/your-org/t4dailydriver:cu121-pt231-vllm053-latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        env:
        - name: MODEL_PATH
          value: "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: HF_HOME
          value: "/cache/hf"
        - name: VLLM_WORKDIR
          value: "/cache/vllm"
        - name: DEPLOYMENT_VERSION
          value: "legacy"
        resources:
          requests:
            memory: "16Gi"
            cpu: "4"
            nvidia.com/gpu: "1"
          limits:
            memory: "32Gi"
            cpu: "8"
            nvidia.com/gpu: "1"
        volumeMounts:
        - name: cache
          mountPath: /cache
        - name: model-cache
          mountPath: /model-cache
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 300
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        startupProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30
      volumes:
      - name: cache
        emptyDir:
          sizeLimit: 10Gi
      - name: model-cache
        persistentVolumeClaim:
          claimName: model-cache-pvc
---
# CUDA 12.4 canary deployment (5% traffic)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-cuda124
  namespace: vllm-production
  labels:
    app: vllm
    version: cuda124
    cuda: "12.4"
    pytorch: "2.5.1"
    vllm: "0.9+"
spec:
  replicas: 1  # Start with 1 replica for canary
  selector:
    matchLabels:
      app: vllm
      version: cuda124
  template:
    metadata:
      labels:
        app: vllm
        version: cuda124
        cuda: "12.4"
        pytorch: "2.5.1"
        vllm: "0.9+"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      nodeSelector:
        nvidia.com/gpu: "true"
        # Try to schedule on L4 nodes if available, fallback to T4
        node.kubernetes.io/instance-type: "g5.xlarge"  # L4 GPU nodes preferred
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
        - key: "cuda-upgrade"
          operator: "Equal"
          value: "12.4"
          effect: "PreferNoSchedule"
      containers:
      - name: vllm
        image: ghcr.io/your-org/t4dailydriver:cu124-pt251-vllm09-latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        env:
        - name: MODEL_PATH
          value: "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: HF_HOME
          value: "/cache/hf"
        - name: VLLM_WORKDIR
          value: "/cache/vllm"
        - name: DEPLOYMENT_VERSION
          value: "cuda124"
        - name: PYTORCH_CUDA_ALLOC_CONF
          value: "max_split_size_mb:512"
        - name: CUDA_MODULE_LOADING
          value: "LAZY"
        resources:
          requests:
            memory: "16Gi"
            cpu: "4"
            nvidia.com/gpu: "1"
          limits:
            memory: "32Gi"
            cpu: "8"
            nvidia.com/gpu: "1"
        volumeMounts:
        - name: cache
          mountPath: /cache
        - name: model-cache
          mountPath: /model-cache
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 360
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 90
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        startupProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 45
          periodSeconds: 15
          timeoutSeconds: 5
          failureThreshold: 40  # 10 minutes total
      volumes:
      - name: cache
        emptyDir:
          sizeLimit: 10Gi
      - name: model-cache
        persistentVolumeClaim:
          claimName: model-cache-pvc
---
# Service with traffic splitting
apiVersion: v1
kind: Service
metadata:
  name: vllm-service
  namespace: vllm-production
  annotations:
    service.kubernetes.io/topology-mode: "Auto"
spec:
  selector:
    app: vllm
  ports:
    - port: 80
      targetPort: 8080
      name: http
  type: ClusterIP
---
# Istio VirtualService for canary traffic management
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: vllm-virtualservice
  namespace: vllm-production
spec:
  hosts:
  - vllm-service
  http:
  - match:
    - headers:
        x-canary:
          exact: "true"
    route:
    - destination:
        host: vllm-service
        subset: cuda124
      weight: 100
  - route:
    - destination:
        host: vllm-service
        subset: legacy
      weight: 95
    - destination:
        host: vllm-service
        subset: cuda124
      weight: 5
---
# Istio DestinationRule for subsets
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: vllm-destination-rule
  namespace: vllm-production
spec:
  host: vllm-service
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 100
        h2MaxRequests: 100
        maxRequestsPerConnection: 2
    loadBalancer:
      simple: LEAST_REQUEST
  subsets:
  - name: legacy
    labels:
      version: legacy
    trafficPolicy:
      connectionPool:
        tcp:
          maxConnections: 80
  - name: cuda124
    labels:
      version: cuda124
    trafficPolicy:
      connectionPool:
        tcp:
          maxConnections: 20
---
# PersistentVolumeClaim for model cache
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: model-cache-pvc
  namespace: vllm-production
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Gi
  storageClassName: efs-sc  # Use EFS for shared model cache
---
# HorizontalPodAutoscaler for legacy deployment
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: vllm-legacy-hpa
  namespace: vllm-production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vllm-legacy
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Max
---
# HorizontalPodAutoscaler for CUDA 12.4 deployment
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: vllm-cuda124-hpa
  namespace: vllm-production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vllm-cuda124
  minReplicas: 1
  maxReplicas: 3  # Limited for canary
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 600  # Slower scale-down for canary
      policies:
      - type: Pods
        value: 1
        periodSeconds: 120
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Pods
        value: 1
        periodSeconds: 60
---
# ServiceMonitor for Prometheus
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vllm-service-monitor
  namespace: vllm-production
spec:
  selector:
    matchLabels:
      app: vllm
  endpoints:
  - port: http
    path: /metrics
    interval: 30s
    scrapeTimeout: 10s
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_label_version]
      targetLabel: deployment_version
    - sourceLabels: [__meta_kubernetes_pod_label_cuda]
      targetLabel: cuda_version
    - sourceLabels: [__meta_kubernetes_pod_label_pytorch]
      targetLabel: pytorch_version
    - sourceLabels: [__meta_kubernetes_pod_label_vllm]
      targetLabel: vllm_version
